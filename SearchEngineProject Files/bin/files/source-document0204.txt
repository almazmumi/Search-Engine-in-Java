 Data warehousing procedures usually subdivide a big ETL process into smaller pieces running sequentially or in parallel. To keep track of data flows, it makes sense to tag each data row with "row_id", and tag each piece of the process with "run_id". In case of a failure, having these IDs help to roll back and rerun the failed piece. Best practice also calls for checkpoints, which are states when certain phases of the process are completed. Once at a checkpoint, it is a good idea to write everything to disk, clean out some temporary files, log the state, and so on. Virtual ETL[edit] As of 2010 data virtualization had begun to advance ETL processing. The application of data virtualization to ETL allowed solving the most common ETL tasks of data migrationand application integration for multiple dispersed data sources. So-called Virtual ETL operates with the abstracted representation of the objects or entities gathered from the variety of relational, semi-structured and unstructured data sources. ETL tools can leverage object-oriented modeling and work with entities' representations persistently stored in a centrally located hub-and-spoke architecture. Such a collection that contains representations of the entities or objects gathered from the data sources for ETL processing is called a metadata repository and it can reside in memory[2] or be made persistent. By using a persistent metadata repository, ETL tools can transition from one-time projects to persistent middleware, performing data harmonization and data profiling consistently and in near-real time.[citation needed] Dealing with keys[edit] Keys are some of the most important objects in all relational databases, as they tie everything together. A primary key is a column that identifies a given entity, where a foreign key is a column in another table that refers a primary key. These keys can also be made of several columns, in which case they are composite keys. In many cases the primary key is an auto generated integer that has no meaning for the business entity being represented, but solely exists for the purpose of the relational database - commonly referred to as a surrogate key. As there is usually more than one data source being loaded into the warehouse, the keys are an important concern to be addressed. Your customers might be represented in several data sources, and in one their (Social Security Number) might be the primary key, their phone number in another and a surrogate in the third. All of the customers information needs to be consolidated into one dimension table. A recommended way to deal with the concern is to add a warehouse surrogate key, which is used as a foreign key from the fact table.[3] Usually updates occur to a dimension's source data, which obviously must be reflected in the data warehouse. If the primary key of the source data is required for reporting, the dimension already contains that piece of information for each row. If the source data uses a surrogate key, the warehouse must keep track of it even though it is never used in queries or reports. That is done by creating a lookup table that contains the warehouse surrogate key and the originating key.[4] This way the dimension is not polluted with surrogates from various source systems, while the ability to update is preserved. The lookup table is used in different ways depending on the nature of the source data. There are 5 types to consider,[5] where three selected ones are included here: Type 1: - The dimension row is simply updated to match the current state of the source system. The warehouse does not capture history. The lookup table is used to identify the dimension row to update or overwrite. Type 2: - A new dimension row is added with the new state of the source system. A new surrogate key is assigned. Source key is no longer unique in the lookup table. Fully logged: - A new dimension row is added with the new state of the source system, while the previous dimension row is updated to reflect it is no longer active and record time of deactivation. Tools[edit] Programmers can set up ETL processes using almost any programming language, but building such processes from scratch can become complex. Increasingly, companies are buying ETL tools to help in the creation of ETL processes.[6] By using an established ETL framework, one may increase one's chances of ending up with better connectivity and scalability.[citation needed] A good ETL tool must be able to communicate with the many different relational databases and read the various file formats used throughout an organization. ETL tools have started to migrate into Enterprise Application Integration, or even Enterprise Service Bus, systems that now cover much more than just the extraction, transformation, and loading of data. Many ETL vendors now have data profiling, data quality, and metadata capabilities. A common use case for ETL tools include converting CSV files to formats readable by relational databases. A typical translation of millions of records is facilitated by ETL tools that enable users to input csv-like data feeds/files and import it into a database with as little code as possible. ETL Tools are typically used by a broad range of professionals - from students in computer science looking to quickly import large data sets to database architects in charge of company account management, ETL Tools have become a convenient tool that can be relied on to get maximum performance. ETL tools in most cases contain a GUI that helps users conveniently transform data as opposed to writing large programs to parse files and modify data types—which ETL tools facilitate as much as possible.[citation needed] Enterprise integration is a technical field of Enterprise Architecture, which focused on the study of topics such as system interconnection, electronic data interchange, product data exchange and distributed computing environments.[1] It is a concept in Enterprise engineering to provide the right information at the right place and at the right time and thereby enable communication between people, machines and computers and their efficient co-operation and co-ordination.[2] Requirements and principles deal with determining the business drivers and guiding principles that help in the development of the enterprise architecture. Each functional and non-functional requirement should be traceable to one or more business drivers. Organizations are beginning to become more aware of the need for capturing and managing requirements. Use-case modeling is one of the techniques that is used for doing this. Enterprise Integration, according Brosey et al. (2001), "aims to connect and combines people, processes, systems, and technologies to ensure that the right people and the right processes have the right information and the right resources at the right time".[3] Enterprise Integration is focused on optimizing operations in a world which could be considered full of continuous and largely unpredictable change. Changes occur in single manufacturing companies just as well as in an "everchanging set of extended or virtual enterprises". It enables the actors to make "quick and accurate decisions and adaptation of operations to respond to emerging threats and opportunities".[3] Enterprise integration has been discussed since the early days of computers in industry and especially in the manufacturing industry with Computer Integrated Manufacturing (CIM) as the acronym for operations integration. In spite of the different understandings of the scope of integration in CIM it has always stood for information integration across at least parts of the enterprise. Information integration essentially consists of providing the right information, at the right place, at the right time.[4] In the 1990s enterprise integration and enterprise engineering became a focal point of discussions with active contribution of many disciplines. The state of the art in enterprise engineering and integration by the end of the 1990s has been rather confusing, according to Jim Nell and Kurt Kosanke (1997): On one hand, it claims to provide solutions for many of the issues identified in enterprise integration. On the other hand, the solutions seem to compete with each other, Neutral networks are a subset of the sequences in sequence space that have equivalent function, and so form a wide, flat plateau in a fitness landscape. Neutral evolution can therefore be visualised as a population diffusing from one set of sequence nodes, through the neutral network, to another cluster of sequence nodes.use conflicting terminology and do not provide any clues on their relations to solutions on other issues. Workflow modelling, business process modelling, business process reengineering (BPR), and concurrent engineering all aim toward identifying and providing the information needed in the enterprise operation. In addition, numerous integrating-platforms concepts are promoted with only marginal or no recognition or support of information identification. Tools claiming to supportenterprise modelling exist in very large numbers, but the support is rather marginal, especially if models are to be used by the end user, for instance, in decision support. Enterprise integration topics[edit] Enterprise modeling[edit] In his 1996 book "Enterprise Modeling and Integration: Principles and Applications" François Vernadat states, that "enterprise modeling is concerned with assessing various aspects of an enterprise in order to better understand, restructure or design enterprise operations. It is the basis of business process reengineering and the first step to achieving enterprise integration. Enterprise integration according to Vernadat is a rapidly developing technical field which has already shown proven solutions for system interconnection, electronic data interchange, product data exchange and distributed computing environments. His book combines these two methodologies and advocates a systematic engineering approach called Enterprise Engineering, for modeling, analysing, designing and implementing integrated enterprise systems".[5] Enterprise integration needs[edit] With this understanding the different needs in enterprise integration can be identified:[4] Identify the right information: requires a precise knowledge of the information needed and created by the different activities in the enterprise operation. Knowledge has to be structured in the form of an accurate model of the enterprise operation, which describes product and administrative information, resources and organisational aspects of the operational processes and allows what-if analysis in order to optimize these processes. Provide the right information at the right place: requires information sharing systems and integration platforms capable of handling information transaction across heterogeneous environments consisting of heterogeneous hardware, different operating systems and monolithic software applications (legacy systems). Environments which cross organizational boundaries and link the operation of different organisations on a temporal basis and with short set-up times and limited time horizon (extended and virtual enterprises). Update the information in real time to reflect the actual state of the enterprise operation: requires not only the up-date of the operational data (information created during the operation), but adapting to environmental changes, which may originate from new customer demands, new technology, new legislation or new philosophies of the society at large. Changes may require modification of the operational processes, the human organization or even the overall scope and goals of the enterprise. Coordinate business processes: requires precise modelling of the enterprise operation in terms of business processes, their relations with each other, with information, resources and organisation. This goes far beyond exchange of information and information sharing. It takes into account decisional capabilities and know-how within the enterprise for real time decision support and evaluation of operational alternatives. Organize and adapt the enterprise: requires very detailed and up-to-date knowledge of both the current state of the enterprise operation and its environment (market, technology, society). Knowledge has to be available a priori and very well structured to allow easy identification of and access to relevant information. Explicit knowledge on information needs during the operation of the enterprise can be provided by a model of the operational processes. A model which identifies the operational tasks, their required information supply and removal needs as well as the point in time of required information transactions. In order to enable consistent modelling of the enterprise operation the modelling process has to be guided and supported by a reference architecture, a methodology and IT based tools.[6] The Generalised Enterprise Reference Architecture and Methodology (GERAM) framework defined by the IFAC/IFIP Task Force provides the necessary guidance of the modelling process, see figure, and enables semantic unification of the model contents as well. The framework identifies the set of components necessary and helpful for enterprise modelling. The general concepts identified and defined in the reference architecture consist of life cycle, life history, model views among others. These concept help the user to create and maintain the process models of the operation and use them in her/his daily work. The modelling tools will support both model engineering and model use by providing an appropriate methodology and language for guiding the user and model representation, respectively.[6] Transfer of information[edit] To enable an integrated real time support of the operation, both the process descriptions and the actual information have to be available in real time for decision support, operation monitoring and control, and model maintenance.[6] The figure illustrates the concept of an integrating infrastructure linking the enterprise model to the real world systems. Integrating services act as a harmonising platform across the heterogeneous system environments (IT and others) and provide the necessary execution support for the model. The process dynamics captured in the enterprise model act as the control flow for model enactment. Therefore access to information and its transfer to and from the location of use is controlled by the model and supported by the integrating infrastructure. The harmonising characteristics of the integrating infrastructure enables transfer of information across and beyond the organisation. Through the semantic unification of the modelling framework interoperability of enterprise models is assured as well.[6] Enterprise Integration Act of 2002[edit] The Public Law 107-277 (116 Stat. 1936-1938), known as the Enterprise Integration Act of 2002, authorizes the National Institute of Standards and Technology to work with major manufacturing industries on an initiative of standards development and implementation for electronic enterprise integration, etc. It requires the Director of the National Institute of Standards and Technology (NIST) to establish an initiative for advancing enterprise integration within the United States which shall:[7] involve the various units of NIST, including NIST laboratories, the Manufacturing Extension Partnership program, and the Baldrige Performance Excellence Program, and consortia that include government and industry; build upon ongoing efforts of NIST and the private sector; and address the enterprise integration needs of each major U.S. manufacturing industry at the earliest possible date. 