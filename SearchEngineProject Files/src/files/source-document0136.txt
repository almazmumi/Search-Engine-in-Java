Computational complexity theory models randomized algorithms as probabilistic Turing machines. Both Las Vegas and Monte Carlo algorithms are considered, and several complexity classes are studied. The most basic randomized complexity class is RP, which is the class of decision problems for which there is an efficient (polynomial time) randomized algorithm (or probabilistic Turing machine) which recognizes NO-instances with absolute certainty and recognizes YES-instances with a probability of at least 1/2. The complement class for RP is co-RP. Problem classes having (possibly nonterminating) algorithms with polynomial time average case running time whose output is always correct are said to be in ZPP. The class of problems for which both YES and NO-instances are allowed to be identified with some error is called BPP. This class acts as the randomized equivalent of P, i.e. BPP represents the class of efficient randomized algorithms. History[edit] Historically, the first randomized algorithm was a method developed by Michael O. Rabin for the closest pair problem in computational geometry.[3] The study of randomized algorithms was spurred by the 1977 discovery of a randomized primality test (i.e., determining the primality of a number) by Robert M. Solovay and Volker Strassen. Soon afterwards Michael O. Rabin demonstrated that the 1976 Miller's primality test can be turned into a randomized algorithm. At that time, no practical deterministic algorithm for primality was known. The Miller-Rabin primality test relies on a binary relation between two positive integers k and n that can be expressed by saying that k "is a witness to the compositeness of" n. It can be shown that If there is a witness to the compositeness of n, then n is composite (i.e., n is not prime), and If n is composite then at least three-fourths of the natural numbers less than n are witnesses to its compositeness, and There is a fast algorithm that, given k and n, ascertains whether k is a witness to the compositeness of n. Observe that this implies that the primality problem is in Co-RP. If one randomly chooses 100 numbers less than a composite number n, then the probability of failing to find such a "witness" is (1/4)100 so that for most practical purposes, this is a good primality test. If n is big, there may be no other test that is practical. The probability of error can be reduced to an arbitrary degree by performing enough independent tests. Therefore, in practice, there is no penalty associated with accepting a small probability of error, since with a little care the probability of error can be made astronomically small. Indeed, even though a deterministic polynomial-time primality test has since been found (see AKS primality test), it has not replaced the older probabilistic tests in cryptographic software nor is it expected to do so for the foreseeable future.Quicksort is a familiar, commonly used algorithm in which randomness can be useful. Any deterministic version of this algorithm requires O(n2) time to sort n numbers for some well-defined class of degenerate inputs (such as an already sorted array), with the specific class of inputs that generate this behavior defined by the protocol for pivot selection. However, if the algorithm selects pivot elements uniformly at random, it has a provably high probability of finishing in O(n log n) time regardless of the characteristics of the input. Randomized incremental constructions in geometry[edit] In computational geometry, a standard technique to build a structure like a convex hull or Delaunay triangulation is to randomly permute the input pointThat links the diverse fields of neuroscience, cognitive science, and psychology with electrical engineering, science, mathematics, and physics.Computational neuroscience is distinct from psychological connectionism and from learning theories of disciplines such as machine learning, neural networks, and computational learning theory in that it emphasizes descriptions of functional and biologically realistic neurons (and neural systems) and their physiology and dynamics.s and then insert them one by one into the existing structure. The randomization ensures that the expected number of changes to the structure caused by an insertion is small, and so the expected running time of the algorithm can be upper bounded. This technique is known as randomized incremental construction.In each execution of the outer loop, the algorithm repeats the inner loop until only 2 nodes remain, the corresponding cut is obtained. The run time of one execution is O(n), and n denotes the number of vertices. After m times executions of the outer loop, we output the minimum cut among all the results. The figure 2 gives an example of one execution of the algorithm. After execution, we get a cut of size 3. Lemma 1: Let k be the min cut size, and let C = {e1,e2,...,ek} be the min cut. If, during iteration i, no edge e ∈ C is selected for contraction, then Ci = C. Proof: If G is not connected, then G can be partitioned into L and R without any edge between them. So the min cut in a disconnected graph is 0. Now, assume G is connected. Let V=L∪ R be the partition of V induced by C : C={ {u,v} ∈ E : u ∈ L,v ∈ R } (well-defined since G is connected). Consider an edge {u,v} of C. Initially, u,v are distinct vertices. As long as we pick an edge f ≠ e, u and v do not get merged. Thus, at the end of the algorithm, we have two compound nodes covering the entire graph, one consisting of the vertices of L and the other consisting of the vertices of R. As in figure 2, the size of min cut is 1, and C = {(A,B)}. If we don't select (A,B) for contraction, we can get the min cut. Lemma 2: If G is a multigraph with p vertices and whose min cut has size k, then G has at least pk/2 edges. Proof: Because the min cut is k, every vertex v must satisfy degree(v) ≥ k. Therefore, the sum of the degree is at least pk. But it is well known that the sum of vertex degrees equals 2|E|. The lemma follows. Analysis of algorithm The probability that the algorithm succeeds is 1 − the probability that all attempts fail. By independence, the probability that all attempts fail is \prod_{i=1}^m \Pr(C_i\neq C)=\prod_{i=1}^m(1-\Pr(C_i=C)). By lemma 1, the probability that Ci = C is the probability that no edge of C is selected during iteration i. Consider the inner loop and let Gj denote the graph after j edge contractions, where j ∈ {0,1,...,n − 3}. Gj has n − j vertices. We use the chain rule of conditional possibilities. The probability that the edge chosen at iteration j is not in C, given that no edge of C has been chosen before, is 1-\frac{k}{|E(G_j)|}. Note that Gj still has min cut of size k, so by Lemma 2, it still has at least \frac{(n-j)k}{2} edges. Thus, 1-\frac{k}{|E(G_j)|}\geq 1-\frac{2}{n-j}=\frac{n-j-2}{n-j}. So by the chain rule, the probability of finding the min cut C is Pr[C_i=C] \geq \left(\frac{n-2}{n}\right)\left(\frac{n-3}{n-1}\right)\left(\frac{n-4}{n-2}\right)\ldots\left(\frac{3}{5}\right)\left(\frac{2}{4}\right)\left(\frac{1}{3}\right). Cancellation gives \Pr[C_i=C]\geq \frac{2}{n(n-1)}. Thus the probability that the algorithm succeeds is at least 1-\left(1-\frac{2}{n(n-1)}\right)^m. For m=\frac{n(n-1)}{2}\ln n, this is equivalent to 1-\frac{1}{n}. The algorithm finds the min cut with probability 1-\frac{1}{n}, in time O(mn)=O(n^3\log n).