Randomness can be viewed as a resource, like space and time. Derandomization is then the process of removing randomness (or using as little of it as possible). From the viewpoint of computational complexity, derandomizing an efficient randomized algorithm is the question, is P = BPP ? There are also specific methods that can be employed to derandomize particular randomized algorithms: the method of conditional probabilities, and its generalization, pessimistic estimators discrepancy theory (which is used to derandomize geometric algorithms) the exploitation of limited independence in the random variables used by the algorithm, such as the pairwise independence used in universal hashing the use of expander graphs (or dispersers in general) to amplify a limited amount of initial randomness (this last approach is also referred to as generating pseudorandom bits from a random source, and leads to the related topic of pseudorandomness) Where randomness helps[edit] When the model of computation is restricted to Turing machines, it is currently an open question whether the ability to make random choices allows some problems to be solved in polynomial time that cannot be solved in polynomial time without this ability; this is the question of whether P = BPP. However, in other contexts, there are specific examples of problems where randomization yields strict improvements. Based on the initial motivating example: given an exponentially long string of 2k characters, half a's and half b's, a random access machine requires at least 2k−1 lookups in the worst-case to find the index of an a; if it is permitted to make random choices, it can solve this problem in an expected polynomial number of lookups. The natural way of carrying out a numerical computation in embedded systems or cyber-physical systems is to provide a result that approximates the correct one with high probability -or Probably Approximately Correct Computation (PACC)-. The hard problem associated with the evaluation of the discrepancy loss between the approximated and the correct computation can be effectively addressed by resorting to randomization [7] In communication complexity, the equality of two strings can be verified to some reliability using \log n bits of communication with a randomized protocol. Any deterministic protocol requires \Theta(n) bits if defending against a strong opponent.[8] The volume of a One key thing during information storage in computer is remembered that is data independence mean data should not be directly accessible that exist in list, arrays or pointers rather then it should provide interface to the applications. Conventional database store information on the base of content that is to be store or store data on the base of their connected data.convex body can be estimated by a randomized algorithm to arbitrary precision in polynomial time.[9] Bárány and Füredi showed that no deterministic algorithm can do the same.[10] This is true unconditionally, i.e. without relying on any complexity-theoretic assumptions. A more complexity-theoretic example of a place where randomness appears to help is the class IP. IP consists of all languages that can be accepted (with high probability) by a polynomially long interaction between an all-powerful prover and a verifier that implements a BPP algorithm. IP = PSPACE.[11] However, if it is required that the verifier be deterministic, then IP = NP. In a chemical reaction network (a finite set of reactions like A+B → 2C + D operating on a finite number of molecules), the ability to ever reach a given target state from an initial state is decidable, while even approximating the probability of ever reaching a given target state (using the standard concentration-based probability for which reaction will occur next) is undecidable.[citation needed] More specifically, a limited Turing machine can be simulated with arbitrarily high probability of running correctly for all time, only if a random chemical reaction network is used.[citation needed] With a simple nondeterministic chemical reaction network (any possible reaction can happen next), the computational power is limited to primitive recursive functions.